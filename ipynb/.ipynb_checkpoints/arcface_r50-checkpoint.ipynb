{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb0068a",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30896353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim import SGD\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bec80",
   "metadata": {},
   "source": [
    "### backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c59347",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['iresnet18', 'iresnet34', 'iresnet50']\n",
    "using_ckpt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd093f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 Conv\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, \n",
    "                     out_planes, \n",
    "                     kernel_size=3, \n",
    "                     stride=stride, \n",
    "                     groups=groups, \n",
    "                     bias=False, \n",
    "                     dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a14aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x1 Conv\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, \n",
    "                     out_planes, \n",
    "                     kernel_size=1, \n",
    "                     stride=stride, \n",
    "                     bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbc6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBasicBlock\n",
    "class IBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, \n",
    "                 groups=1, base_width=64, dilation=1):\n",
    "        super(IBasicBlock, self).__init__()\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError('Dilation > 1 not supported in BasicBlock')\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05)\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05)\n",
    "        self.prelu = nn.PReLU(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward_impl(self, x):\n",
    "        identity = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.prelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training and using_ckpt:\n",
    "            return checkpoint(self.forward_impl, x)\n",
    "        else:\n",
    "            return self.forward_impl(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f03edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IResNet\n",
    "class IResNet(nn.Module):\n",
    "    fc_scale = 7 * 7\n",
    "    def __init__(self, block, layers, dropout=0, num_features=512, zero_init_residual=False,\n",
    "                groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n",
    "        super(IResNet, self).__init__()\n",
    "        self.extra_gflops = 0.0\n",
    "        self.fp16 = fp16\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        \n",
    "        if replace_stride_with_dilation is None:\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None\"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        \n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n",
    "        self.prelu = nn.PReLU(self.inplanes)\n",
    "        self.layer1 = self._make_layer(block, \n",
    "                                       64, \n",
    "                                       layers[0], \n",
    "                                       stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, \n",
    "                                       layers[1], \n",
    "                                       stride=2, \n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, \n",
    "                                       layers[2], \n",
    "                                       stride=2, \n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, \n",
    "                                       layers[3], \n",
    "                                       stride=2, \n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05)\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=True)\n",
    "        self.fc = nn.Linear(512 * block.expansion * self.fc_scale, num_features)\n",
    "        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n",
    "        nn.init.constant_(self.features.weight, 1.0)\n",
    "        self.features.weight.requires_grad = False\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, IBasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "                    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "            conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "            nn.BatchNorm2d(planes * block.expansion, eps=1e-05))\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                 self.base_width, previous_dilation))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(self.inplanes, \n",
    "                      planes, \n",
    "                      groups=self.groups, \n",
    "                      base_width=self.base_width, \n",
    "                      dilation=self.dilation))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.cuda.amp.autocast(self.fp16):\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.prelu(x)\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            x = self.bn2(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x.float() if self.fp16 else x)\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement iresnet\n",
    "def _iresnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = IResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        raise ValueError()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7ccc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iresnet18\n",
    "def iresnet18(pretrained=False, progress=True, **kwargs):\n",
    "    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad60284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iresnet34\n",
    "def iresnet34(pretrained=False, progress=True, **kwargs):\n",
    "    return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be25193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iresnet50\n",
    "def iresnet50(pretrained=False, progress=True, **kwargs):\n",
    "    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a18f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "def get_model(name, **kwargs):\n",
    "    # resnet\n",
    "    if name == 'r18':\n",
    "        return iresnet18(False, **kwargs)\n",
    "    elif name == 'r34':\n",
    "        return iresnet34(False, **kwargs)\n",
    "    elif name == 'r50':\n",
    "        return iresnet50(False, **kwargs)\n",
    "    else:\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011f61a",
   "metadata": {},
   "source": [
    "### losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38fe1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Margin Loss\n",
    "class CombinedMarginLoss(torch.nn.Module):\n",
    "    def __init__(self, s, m1, m2, m3, interclass_filtering_threshold=0):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.m3 = m3\n",
    "        self.interclass_filtering_threshold = interclass_filtering_threshold\n",
    "        \n",
    "        # For ArcFace\n",
    "        self.cos_m = math.cos(self.m2)\n",
    "        self.sin_m = math.sin(self.m2)\n",
    "        self.theta = math.cos(math.pi - self.m2)\n",
    "        self.sinmm = math.sin(math.pi - self.m2) * self.m2\n",
    "        self.easy_margin = False\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        index_positive = torch.where(labels != -1)[0]\n",
    "\n",
    "        if self.interclass_filtering_threshold > 0:\n",
    "            with torch.no_grad():\n",
    "                dirty = logits > self.interclass_filtering_threshold\n",
    "                dirty = dirty.float()\n",
    "                mask = torch.ones([index_positive.size(0), logits.size(1)], device=logits.device)\n",
    "                mask.scatter_(1, labels[index_positive], 0)\n",
    "                dirty[index_positive] *= mask\n",
    "                tensor_mul = 1 - dirty    \n",
    "            logits = tensor_mul * logits\n",
    "\n",
    "        target_logit = logits[index_positive, labels[index_positive].view(-1)]\n",
    "\n",
    "        if self.m1 == 1.0 and self.m3 == 0.0:\n",
    "            with torch.no_grad():\n",
    "                target_logit.arccos_()\n",
    "                logits.arccos_()\n",
    "                final_target_logit = target_logit + self.m2\n",
    "                logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n",
    "                logits.cos_()\n",
    "            logits = logits * self.s        \n",
    "\n",
    "        elif self.m3 > 0:\n",
    "            final_target_logit = target_logit - self.m3\n",
    "            logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n",
    "            logits = logits * self.s\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aec5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArcFace\n",
    "class ArcFace(torch.nn.Module):\n",
    "    \"\"\" ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf) \"\"\"\n",
    "    def __init__(self, s=64.0, margin=0.5):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.s = s\n",
    "        self.margin = margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.theta = math.cos(math.pi - margin)\n",
    "        self.sinmm = math.sin(math.pi - margin) * margin\n",
    "        self.easy_margin = False\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logit = logits[index, labels[index].view(-1)]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_logit.arccos_()\n",
    "            logits.arccos_()\n",
    "            final_target_logit = target_logit + self.margin\n",
    "            logits[index, labels[index].view(-1)] = final_target_logit\n",
    "            logits.cos_()\n",
    "        logits = logits * self.s\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44076ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CosFace\n",
    "class CosFace(torch.nn.Module):\n",
    "    def __init__(self, s=64.0, m=0.40):\n",
    "        super(CosFace, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logit = logits[index, labels[index].view(-1)]\n",
    "        final_target_logit = target_logit - self.m\n",
    "        logits[index, labels[index].view(-1)] = final_target_logit\n",
    "        logits = logits * self.s\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd42a65",
   "metadata": {},
   "source": [
    "### lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de81406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialLRWarmup(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_iters, total_iters=5, power=1.0, last_epoch=-1, verbose=False):\n",
    "        super().__init__(optimizer, last_epoch=last_epoch, verbose=verbose)\n",
    "        self.total_iters = total_iters\n",
    "        self.power = power\n",
    "        self.warmup_iters = warmup_iters\n",
    "\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0 or self.last_epoch > self.total_iters:\n",
    "            return [group[\"lr\"] for group in self.optimizer.param_groups]\n",
    "\n",
    "        if self.last_epoch <= self.warmup_iters:\n",
    "            return [base_lr * self.last_epoch / self.warmup_iters for base_lr in self.base_lrs]\n",
    "        else:        \n",
    "            l = self.last_epoch\n",
    "            w = self.warmup_iters\n",
    "            t = self.total_iters\n",
    "            decay_factor = ((1.0 - (l - w) / (t - w)) / (1.0 - (l - 1 - w) / (t - w))) ** self.power\n",
    "        return [group[\"lr\"] * decay_factor for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "\n",
    "        if self.last_epoch <= self.warmup_iters:\n",
    "            return [\n",
    "                base_lr * self.last_epoch / self.warmup_iters for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [\n",
    "                (\n",
    "                    base_lr * (1.0 - (min(self.total_iters, self.last_epoch) - self.warmup_iters) / (self.total_iters - self.warmup_iters)) ** self.power\n",
    "                )\n",
    "                for base_lr in self.base_lrs\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e312c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    class TestModule(torch.nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.linear = torch.nn.Linear(32,32)\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    test_module = TestModule()\n",
    "test_module_pfc = TestModule()\n",
    "lr_pfc_weight = 1/3\n",
    "base_lr = 10\n",
    "total_steps = 1000\n",
    "\n",
    "sgd = SGD([\n",
    "    {\"params\" : test_module.parameters(), \"lr\": base_lr},\n",
    "    {\"params\" : test_module_pfc.parameters(), \"lr\": base_lr * lr_pfc_weight}\n",
    "], base_lr)\n",
    "\n",
    "scheduler = PolynomialLRWarmup(sgd, total_steps//10, total_steps, power=2)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "y_pfc = []\n",
    "\n",
    "for i in range(total_steps):\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    lr_pfc = scheduler.get_last_lr()[1]\n",
    "    x.append(i)\n",
    "    y.append(lr)\n",
    "    y_pfc.append(lr_pfc)\n",
    "\n",
    "fontsize=15\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(x, y, linestyle='-', linewidth=2, )\n",
    "plt.plot(x, y_pfc, linestyle='-', linewidth=2, )\n",
    "plt.xlabel('Iterations')     # x_label\n",
    "plt.ylabel(\"Lr\")             # y_label\n",
    "plt.savefig(\"tmp.png\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbcaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KOR\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ff7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
